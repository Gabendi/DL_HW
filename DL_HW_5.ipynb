{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabendi/DL_HW/blob/master/DL_HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOUXFIDS2O_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.layers import Input, Dense, Lambda, Layer, Multiply, Add\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySDR-wir3OKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train),(x_test,y_test) = cifar10.load_data() #loading data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8mjRsdl35_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0OrN1H64An3",
        "colab_type": "code",
        "outputId": "a786695b-f250-48b0-b89a-f3c8e3257417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(x_train[25])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdcc8288160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAACOCAYAAADn/TAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njkMiOMJxSARHOA6J8P+BuRAPr2/VrwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOBfrc1x4CjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sort_order_train = y_train.argsort(axis=0)\n",
        "sort_order_test = y_test.argsort(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUK0q4w64aBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train[sort_order_train] #sorting by the labels and then choosing one\n",
        "x_train = x_train[sort_order_train]\n",
        "\n",
        "y_test = y_test[sort_order_test]\n",
        "x_test = x_test[sort_order_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmanJz8L6pjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.reshape((50000,1))\n",
        "x_train = x_train.reshape((50000,32,32,3))\n",
        "\n",
        "y_test = y_test.reshape((10000,1))\n",
        "x_test = x_test.reshape((10000,32,32,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9nphyvh5L1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "typ = 6 #choosing the label of frog\n",
        "begin_train_frogs = 0\n",
        "begin_t_f = False\n",
        "end_train_frogs = 0\n",
        "\n",
        "begin_test_frogs = 0\n",
        "begin_test_f = False\n",
        "end_test_frogs = 0\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  if y_train[i] == typ:\n",
        "    if begin_t_f == False:\n",
        "      begin_t_f = True\n",
        "      begin_train_frogs = i\n",
        "    end_train_frogs = i\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i] == typ:\n",
        "    if begin_test_f == False:\n",
        "      begin_test_f = True\n",
        "      begin_test_frogs = i\n",
        "    end_test_frogs = i\n",
        "\n",
        "x_test = x_test[begin_test_frogs:end_test_frogs]\n",
        "x_train = x_train[begin_train_frogs:end_train_frogs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZg02jJ6A3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 126\n",
        "original_dim = 1024*3 # shape of orig. pics\n",
        "latent_dim = 80 # latent dim: 2D\n",
        "intermediate_dim = 256 \n",
        "epochs = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4an_4nx-y9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KLDivergenceLayer(Layer):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        mu, log_var = inputs\n",
        "\n",
        "        kl_batch = - .5 * K.sum(1 + log_var -\n",
        "                                K.square(mu) -\n",
        "                                K.exp(log_var), axis=-1)\n",
        "\n",
        "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
        "\n",
        "        return inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWKteKNR-zuE",
        "colab_type": "code",
        "outputId": "59326a0d-c861-4bd9-e763-f57ecab12ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "x = Input(shape=(original_dim,))\n",
        "\n",
        "# hidden layers\n",
        "h = Dense(intermediate_dim, activation='relu')(x)\n",
        "h = Dense(intermediate_dim, activation='relu')(h)\n",
        "h = Dense(intermediate_dim, activation='relu')(h)\n",
        "\n",
        "# output layers, log of var and mu\n",
        "z_mu = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "\n",
        "\n",
        "# KL-divergence \n",
        "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
        "\n",
        "# normalizing variance\n",
        "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
        "\n",
        "# white noise\n",
        "eps = Input(tensor=K.random_normal(shape=(K.shape(x)[0],\n",
        "                                          latent_dim)))\n",
        "\n",
        "# keras Merge rétegek (Multiply, Add) : reparameterization\n",
        "z_eps = Multiply()([z_sigma, eps])\n",
        "z = Add()([z_mu, z_eps])\n",
        "\n",
        "\n",
        "# decoder network\n",
        "decoder = Sequential([\n",
        "    Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
        "    Dense(intermediate_dim, activation='relu'),\n",
        "    Dense(intermediate_dim, activation='relu'),\n",
        "    Dense(intermediate_dim, activation='relu'),\n",
        "    Dense(original_dim, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# the actual prediction\n",
        "x_pred = decoder(z)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dep5WVtr-7Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# log_pθ(x|z)\n",
        "def nll(y_true, y_pred):\n",
        "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
        "\n",
        "    # keras.losses.binary_crossentropy gives the mean\n",
        "    # over the last axis. we require the sum\n",
        "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGB3U0ld-_Ed",
        "colab_type": "code",
        "outputId": "f425a861-6cec-4f00-9ec9-b86e698fa6da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# the model itself\n",
        "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
        "vae.compile(optimizer=Adam(lr=0.0016), loss=nll)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeNd-zgL_B8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(-1, original_dim) / 255. #preparing the data\n",
        "x_test = x_test.reshape(-1, original_dim) / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4VCFlHe_bGA",
        "colab_type": "code",
        "outputId": "a2286e0a-388f-4bac-b028-6341469b6603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4999, 3072)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL4mlghk_NWn",
        "colab_type": "code",
        "outputId": "52d35184-fb1a-49fa-9c90-22f5856aa524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vae.fit(x_train,\n",
        "        x_train,\n",
        "        shuffle=True,\n",
        "        epochs=150,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, x_test)) #training"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 4999 samples, validate on 999 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "4999/4999 [==============================] - 6s 1ms/step - loss: 2046.5087 - val_loss: 1989.5056\n",
            "Epoch 2/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1983.4679 - val_loss: 1981.7036\n",
            "Epoch 3/150\n",
            "4999/4999 [==============================] - 1s 138us/step - loss: 1977.5390 - val_loss: 1970.2047\n",
            "Epoch 4/150\n",
            "4999/4999 [==============================] - 1s 138us/step - loss: 1966.4817 - val_loss: 1956.7671\n",
            "Epoch 5/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1956.0245 - val_loss: 1950.0556\n",
            "Epoch 6/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1947.1417 - val_loss: 1942.6071\n",
            "Epoch 7/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1943.5804 - val_loss: 1935.5050\n",
            "Epoch 8/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1932.9590 - val_loss: 1929.6313\n",
            "Epoch 9/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1930.0528 - val_loss: 1925.8525\n",
            "Epoch 10/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1925.1177 - val_loss: 1924.6808\n",
            "Epoch 11/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1923.0222 - val_loss: 1917.9157\n",
            "Epoch 12/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1917.5609 - val_loss: 1913.0730\n",
            "Epoch 13/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1915.4970 - val_loss: 1911.7683\n",
            "Epoch 14/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1912.6084 - val_loss: 1909.1893\n",
            "Epoch 15/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1909.6661 - val_loss: 1907.1944\n",
            "Epoch 16/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1907.8267 - val_loss: 1907.1671\n",
            "Epoch 17/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1907.2746 - val_loss: 1904.9766\n",
            "Epoch 18/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1904.8149 - val_loss: 1902.6852\n",
            "Epoch 19/150\n",
            "4999/4999 [==============================] - 1s 129us/step - loss: 1903.0810 - val_loss: 1901.4374\n",
            "Epoch 20/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1901.2788 - val_loss: 1903.9287\n",
            "Epoch 21/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1900.9831 - val_loss: 1898.5155\n",
            "Epoch 22/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1900.2527 - val_loss: 1897.6616\n",
            "Epoch 23/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1898.1370 - val_loss: 1897.1943\n",
            "Epoch 24/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1897.1274 - val_loss: 1898.0227\n",
            "Epoch 25/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1898.4798 - val_loss: 1896.5591\n",
            "Epoch 26/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1894.6484 - val_loss: 1894.0195\n",
            "Epoch 27/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1893.4899 - val_loss: 1893.5346\n",
            "Epoch 28/150\n",
            "4999/4999 [==============================] - 1s 129us/step - loss: 1892.5433 - val_loss: 1892.1110\n",
            "Epoch 29/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1891.1455 - val_loss: 1892.4399\n",
            "Epoch 30/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1890.9988 - val_loss: 1892.0778\n",
            "Epoch 31/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1889.4522 - val_loss: 1890.6839\n",
            "Epoch 32/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1889.1247 - val_loss: 1890.1208\n",
            "Epoch 33/150\n",
            "4999/4999 [==============================] - 1s 138us/step - loss: 1890.6760 - val_loss: 1891.2253\n",
            "Epoch 34/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1888.9101 - val_loss: 1891.3903\n",
            "Epoch 35/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1889.9768 - val_loss: 1890.6808\n",
            "Epoch 36/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1889.7730 - val_loss: 1889.8990\n",
            "Epoch 37/150\n",
            "4999/4999 [==============================] - 1s 129us/step - loss: 1887.3679 - val_loss: 1889.0029\n",
            "Epoch 38/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1887.5035 - val_loss: 1889.8569\n",
            "Epoch 39/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1887.4442 - val_loss: 1888.6990\n",
            "Epoch 40/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1885.9575 - val_loss: 1888.7982\n",
            "Epoch 41/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1886.0916 - val_loss: 1889.4318\n",
            "Epoch 42/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1885.8329 - val_loss: 1888.5481\n",
            "Epoch 43/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1884.4077 - val_loss: 1887.7119\n",
            "Epoch 44/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1884.4093 - val_loss: 1888.1019\n",
            "Epoch 45/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1883.6881 - val_loss: 1886.7962\n",
            "Epoch 46/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1883.2750 - val_loss: 1887.8225\n",
            "Epoch 47/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1883.4443 - val_loss: 1886.9282\n",
            "Epoch 48/150\n",
            "4999/4999 [==============================] - 1s 138us/step - loss: 1882.6158 - val_loss: 1888.7844\n",
            "Epoch 49/150\n",
            "4999/4999 [==============================] - 1s 135us/step - loss: 1882.6769 - val_loss: 1886.3734\n",
            "Epoch 50/150\n",
            "4999/4999 [==============================] - 1s 141us/step - loss: 1883.1506 - val_loss: 1888.1003\n",
            "Epoch 51/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1881.9599 - val_loss: 1887.5666\n",
            "Epoch 52/150\n",
            "4999/4999 [==============================] - 1s 135us/step - loss: 1881.5240 - val_loss: 1889.1616\n",
            "Epoch 53/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1882.1323 - val_loss: 1888.0685\n",
            "Epoch 54/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1881.5982 - val_loss: 1887.4035\n",
            "Epoch 55/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1880.8613 - val_loss: 1887.3038\n",
            "Epoch 56/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1880.1030 - val_loss: 1887.5356\n",
            "Epoch 57/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1881.1136 - val_loss: 1887.9510\n",
            "Epoch 58/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1881.1360 - val_loss: 1888.1627\n",
            "Epoch 59/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1881.1013 - val_loss: 1887.5229\n",
            "Epoch 60/150\n",
            "4999/4999 [==============================] - 1s 129us/step - loss: 1879.9393 - val_loss: 1888.1639\n",
            "Epoch 61/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1879.6019 - val_loss: 1886.7047\n",
            "Epoch 62/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1879.7259 - val_loss: 1886.4078\n",
            "Epoch 63/150\n",
            "4999/4999 [==============================] - 1s 127us/step - loss: 1880.5105 - val_loss: 1886.2851\n",
            "Epoch 64/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1879.9447 - val_loss: 1886.3563\n",
            "Epoch 65/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1879.1775 - val_loss: 1888.4335\n",
            "Epoch 66/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1877.2465 - val_loss: 1885.1941\n",
            "Epoch 67/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1877.9483 - val_loss: 1886.1095\n",
            "Epoch 68/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1878.2543 - val_loss: 1888.3833\n",
            "Epoch 69/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1878.1838 - val_loss: 1886.5946\n",
            "Epoch 70/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1877.0671 - val_loss: 1886.1341\n",
            "Epoch 71/150\n",
            "4999/4999 [==============================] - 1s 129us/step - loss: 1876.6041 - val_loss: 1887.5388\n",
            "Epoch 72/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1876.3993 - val_loss: 1885.6772\n",
            "Epoch 73/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1875.0572 - val_loss: 1886.8793\n",
            "Epoch 74/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1876.5002 - val_loss: 1888.6148\n",
            "Epoch 75/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1876.5411 - val_loss: 1886.5996\n",
            "Epoch 76/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1874.9376 - val_loss: 1887.6748\n",
            "Epoch 77/150\n",
            "4999/4999 [==============================] - 1s 135us/step - loss: 1874.9162 - val_loss: 1885.5469\n",
            "Epoch 78/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1875.5775 - val_loss: 1886.6906\n",
            "Epoch 79/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1873.9029 - val_loss: 1886.8847\n",
            "Epoch 80/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1873.5335 - val_loss: 1886.2724\n",
            "Epoch 81/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1874.7655 - val_loss: 1886.7513\n",
            "Epoch 82/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1874.9311 - val_loss: 1888.3854\n",
            "Epoch 83/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1875.1689 - val_loss: 1886.5212\n",
            "Epoch 84/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1873.6503 - val_loss: 1886.9263\n",
            "Epoch 85/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1872.7937 - val_loss: 1886.0232\n",
            "Epoch 86/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1874.2922 - val_loss: 1886.9868\n",
            "Epoch 87/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1873.1283 - val_loss: 1886.2807\n",
            "Epoch 88/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1871.5822 - val_loss: 1885.8911\n",
            "Epoch 89/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1871.1919 - val_loss: 1887.3376\n",
            "Epoch 90/150\n",
            "4999/4999 [==============================] - 1s 141us/step - loss: 1871.0409 - val_loss: 1886.1636\n",
            "Epoch 91/150\n",
            "4999/4999 [==============================] - 1s 137us/step - loss: 1871.1009 - val_loss: 1886.5348\n",
            "Epoch 92/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1870.7627 - val_loss: 1886.4151\n",
            "Epoch 93/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1870.7499 - val_loss: 1886.5282\n",
            "Epoch 94/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1870.2048 - val_loss: 1886.2151\n",
            "Epoch 95/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1870.1328 - val_loss: 1888.0517\n",
            "Epoch 96/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1870.0766 - val_loss: 1887.3491\n",
            "Epoch 97/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1870.7232 - val_loss: 1886.7531\n",
            "Epoch 98/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1869.9558 - val_loss: 1886.9311\n",
            "Epoch 99/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1870.0656 - val_loss: 1886.8984\n",
            "Epoch 100/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1870.7526 - val_loss: 1888.2883\n",
            "Epoch 101/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1869.5035 - val_loss: 1886.9532\n",
            "Epoch 102/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1868.8704 - val_loss: 1888.0847\n",
            "Epoch 103/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1869.3872 - val_loss: 1890.1169\n",
            "Epoch 104/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1868.9264 - val_loss: 1887.1403\n",
            "Epoch 105/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1867.8821 - val_loss: 1888.2621\n",
            "Epoch 106/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1867.5652 - val_loss: 1887.9447\n",
            "Epoch 107/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1867.2833 - val_loss: 1889.4260\n",
            "Epoch 108/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1867.4771 - val_loss: 1887.7563\n",
            "Epoch 109/150\n",
            "4999/4999 [==============================] - 1s 135us/step - loss: 1867.7762 - val_loss: 1888.7128\n",
            "Epoch 110/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1867.6757 - val_loss: 1892.6366\n",
            "Epoch 111/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1868.5795 - val_loss: 1890.3931\n",
            "Epoch 112/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1868.1461 - val_loss: 1886.7610\n",
            "Epoch 113/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1866.7257 - val_loss: 1887.8078\n",
            "Epoch 114/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1867.1951 - val_loss: 1888.0797\n",
            "Epoch 115/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1866.8576 - val_loss: 1888.8145\n",
            "Epoch 116/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1867.4954 - val_loss: 1892.0529\n",
            "Epoch 117/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1867.7600 - val_loss: 1886.7065\n",
            "Epoch 118/150\n",
            "4999/4999 [==============================] - 1s 135us/step - loss: 1865.4336 - val_loss: 1886.2255\n",
            "Epoch 119/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1866.5565 - val_loss: 1888.3222\n",
            "Epoch 120/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1866.3349 - val_loss: 1887.9850\n",
            "Epoch 121/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1866.3500 - val_loss: 1887.6394\n",
            "Epoch 122/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1867.0591 - val_loss: 1890.8840\n",
            "Epoch 123/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1865.9633 - val_loss: 1889.4409\n",
            "Epoch 124/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1864.7703 - val_loss: 1887.9489\n",
            "Epoch 125/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1864.0790 - val_loss: 1888.1359\n",
            "Epoch 126/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1864.2497 - val_loss: 1889.8400\n",
            "Epoch 127/150\n",
            "4999/4999 [==============================] - 1s 135us/step - loss: 1865.3389 - val_loss: 1888.3534\n",
            "Epoch 128/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1864.0410 - val_loss: 1889.3845\n",
            "Epoch 129/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1864.5515 - val_loss: 1888.1246\n",
            "Epoch 130/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1866.3408 - val_loss: 1894.9750\n",
            "Epoch 131/150\n",
            "4999/4999 [==============================] - 1s 135us/step - loss: 1864.2463 - val_loss: 1891.6805\n",
            "Epoch 132/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1864.0463 - val_loss: 1889.4316\n",
            "Epoch 133/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1863.2054 - val_loss: 1889.6295\n",
            "Epoch 134/150\n",
            "4999/4999 [==============================] - 1s 129us/step - loss: 1863.4484 - val_loss: 1889.3667\n",
            "Epoch 135/150\n",
            "4999/4999 [==============================] - 1s 136us/step - loss: 1863.4485 - val_loss: 1889.1670\n",
            "Epoch 136/150\n",
            "4999/4999 [==============================] - 1s 134us/step - loss: 1863.0170 - val_loss: 1891.6802\n",
            "Epoch 137/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1864.4225 - val_loss: 1893.5189\n",
            "Epoch 138/150\n",
            "4999/4999 [==============================] - 1s 133us/step - loss: 1863.7948 - val_loss: 1889.2853\n",
            "Epoch 139/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1863.3572 - val_loss: 1889.3321\n",
            "Epoch 140/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1863.7491 - val_loss: 1890.3218\n",
            "Epoch 141/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1862.4282 - val_loss: 1889.9021\n",
            "Epoch 142/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1861.6604 - val_loss: 1889.4211\n",
            "Epoch 143/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1861.5868 - val_loss: 1890.8778\n",
            "Epoch 144/150\n",
            "4999/4999 [==============================] - 1s 146us/step - loss: 1861.9213 - val_loss: 1894.8293\n",
            "Epoch 145/150\n",
            "4999/4999 [==============================] - 1s 128us/step - loss: 1863.6835 - val_loss: 1888.6505\n",
            "Epoch 146/150\n",
            "4999/4999 [==============================] - 1s 130us/step - loss: 1863.1323 - val_loss: 1891.6591\n",
            "Epoch 147/150\n",
            "4999/4999 [==============================] - 1s 137us/step - loss: 1862.1410 - val_loss: 1891.1436\n",
            "Epoch 148/150\n",
            "4999/4999 [==============================] - 1s 128us/step - loss: 1861.3259 - val_loss: 1890.3027\n",
            "Epoch 149/150\n",
            "4999/4999 [==============================] - 1s 131us/step - loss: 1861.0136 - val_loss: 1891.6311\n",
            "Epoch 150/150\n",
            "4999/4999 [==============================] - 1s 132us/step - loss: 1860.4502 - val_loss: 1895.0168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcc76fbeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfV39Ia_S1n",
        "colab_type": "code",
        "outputId": "23ec6464-eada-4c67-82fa-aff3e9a4589d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "z_sample = norm.ppf(np.random.rand(1,latent_dim)) #generating a picture\n",
        "generated_digit = decoder.predict(z_sample)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(generated_digit.reshape(32,32,3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdc6ecf6a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAACOCAYAAADn/TAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih_BPtrN_5kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
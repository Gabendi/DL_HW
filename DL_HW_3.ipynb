{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_HW_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabendi/DL_HW/blob/master/DL_HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e7fncSWBG1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "7bcbceaa-1527-4af8-bff1-f49e0501f9eb"
      },
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout\n",
        "from keras.optimizers import SGD,Adam,Adagrad\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHt5Oj3triFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the data\n",
        "df = pd.read_csv('/content/sample_data/budapest.csv', sep=',', usecols=[\"date_time\", \"maxtempC\",\"mintempC\",\"humidity\", \"precipMM\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndk8zQaMsFHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "00db216b-3559-4e3d-b3d8-ac4bf83897e0"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date_time</th>\n",
              "      <th>maxtempC</th>\n",
              "      <th>mintempC</th>\n",
              "      <th>humidity</th>\n",
              "      <th>precipMM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4032</th>\n",
              "      <td>2019-10-25</td>\n",
              "      <td>24</td>\n",
              "      <td>17</td>\n",
              "      <td>51</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4033</th>\n",
              "      <td>2019-10-26</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>59</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4034</th>\n",
              "      <td>2019-10-27</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>57</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4035</th>\n",
              "      <td>2019-10-28</td>\n",
              "      <td>18</td>\n",
              "      <td>13</td>\n",
              "      <td>62</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4036</th>\n",
              "      <td>2019-10-29</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>6.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       date_time  maxtempC  mintempC  humidity  precipMM\n",
              "4032  2019-10-25        24        17        51       0.0\n",
              "4033  2019-10-26        24        16        59       0.0\n",
              "4034  2019-10-27        24        16        57       0.0\n",
              "4035  2019-10-28        18        13        62       0.2\n",
              "4036  2019-10-29        11         0        66       6.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtfBzblsOkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converting date_time into month+day features\n",
        "df.insert(loc=0,column=\"day\",value=df['mintempC'])\n",
        "df.insert(loc=0,column=\"month\",value=df['mintempC'])\n",
        "\n",
        "def getDays(dates):\n",
        "    temp = []\n",
        "\n",
        "    for i in range(0,len(dates)):\n",
        "        temp.append((dates[i].split(sep='-')[2]))\n",
        "\n",
        "    return temp\n",
        "\n",
        "\n",
        "def getMonths(dates):\n",
        "    temp = []\n",
        "\n",
        "    for i in range(0,len(dates)):\n",
        "        temp.append((dates[i].split(sep='-')[1]))\n",
        "\n",
        "    return temp\n",
        "\n",
        "dates = copy.copy(df[\"date_time\"]) #converting date time\n",
        "\n",
        "dates = dates.values\n",
        "\n",
        "df.day = getDays(dates)\n",
        "df.month=getMonths(dates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9i7vVNKs6qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del df['date_time']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ9HvjG7uCSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1e381947-e9cb-4112-f7ae-42a2ddb613f8"
      },
      "source": [
        "Y = copy.copy((df.maxtempC+df.mintempC)/2) #creating the expected target\n",
        "df.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>maxtempC</th>\n",
              "      <th>mintempC</th>\n",
              "      <th>humidity</th>\n",
              "      <th>precipMM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4032</th>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>24</td>\n",
              "      <td>17</td>\n",
              "      <td>51</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4033</th>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>59</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4034</th>\n",
              "      <td>10</td>\n",
              "      <td>27</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>57</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4035</th>\n",
              "      <td>10</td>\n",
              "      <td>28</td>\n",
              "      <td>18</td>\n",
              "      <td>13</td>\n",
              "      <td>62</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4036</th>\n",
              "      <td>10</td>\n",
              "      <td>29</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>6.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     month day  maxtempC  mintempC  humidity  precipMM\n",
              "4032    10  25        24        17        51       0.0\n",
              "4033    10  26        24        16        59       0.0\n",
              "4034    10  27        24        16        57       0.0\n",
              "4035    10  28        18        13        62       0.2\n",
              "4036    10  29        11         0        66       6.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvU9oo3b6hjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "todayData = copy.copy(df.values[-1:len(df)]) #using today's data to predict the upcoming day/week/month"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kixndw6FyPQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123) #\"deterministic\"\n",
        "\n",
        "Xd = copy.copy(df.values[0:-1]) #data for day prediction\n",
        "Yd = copy.copy(Y[1:])\n",
        "\n",
        "randpermd = np.random.permutation(len(Xd)) #permutating\n",
        "\n",
        "\n",
        "Xd = np.array(Xd,dtype='float')[randpermd.astype(int)]\n",
        "Yd = np.array(Yd,dtype='float')[randpermd.astype(int)]\n",
        "\n",
        "Xw = copy.copy(df.values[0:-7]) #data for week prediction\n",
        "Yw = copy.copy(Y[7:])\n",
        "\n",
        "randpermw = np.random.permutation(len(Xw))\n",
        "\n",
        "Xw = np.array(Xw,dtype='float')[randpermw.astype(int)]\n",
        "Yw = np.array(Yw,dtype='float')[randpermw.astype(int)]\n",
        "\n",
        "Xm = copy.copy(df.values[0:-28]) #data for month prediction\n",
        "Ym = copy.copy(Y[28:])\n",
        "\n",
        "\n",
        "randpermm =np.random.permutation(len(Xm))\n",
        "\n",
        "Xm = np.array(Xm,dtype='float')[randpermm.astype(int)]\n",
        "Ym = np.array(Ym,dtype='float')[randpermm.astype(int)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOXQA6Oy6kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_samplesd = len(Yd)\n",
        "nb_samplesw = len(Yw)\n",
        "nb_samplesm = len(Ym)\n",
        "\n",
        "test_split=0.02 #not sure if any use of test split this time \n",
        "\n",
        "\n",
        "#day\n",
        "X_traind = Xd[0:int(nb_samplesd*(1-test_split))]\n",
        "Y_traind = Yd[0:int(nb_samplesd*(1-test_split))]\n",
        "X_testd  = Xd[int(nb_samplesd*(1-test_split)):]\n",
        "Y_testd  = Yd[int(nb_samplesd*(1-test_split)):]\n",
        "\n",
        "# #week\n",
        "\n",
        "X_trainw = Xw[0:int(nb_samplesw*(1-test_split))]\n",
        "Y_trainw = Yw[0:int(nb_samplesw*(1-test_split))]\n",
        "X_testw  = Xw[int(nb_samplesw*(1-test_split)):]\n",
        "Y_testw  = Yw[int(nb_samplesw*(1-test_split)):]\n",
        "\n",
        "#month\n",
        "X_trainm = Xm[0:int(nb_samplesm*(1-test_split))]\n",
        "Y_trainm = Ym[0:int(nb_samplesm*(1-test_split))]\n",
        "X_testm  = Xm[int(nb_samplesm*(1-test_split)):]\n",
        "Y_testm  = Ym[int(nb_samplesm*(1-test_split)):]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6dOrhQcHvma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#standardization\n",
        "#day\n",
        "scalerd = preprocessing.StandardScaler().fit(X_traind)\n",
        "X_traind = scalerd.transform(X_traind)\n",
        "X_testd  = scalerd.transform(X_testd)\n",
        "todayd = copy.copy(scalerd.transform(todayData))\n",
        "#week\n",
        "scalerw = preprocessing.StandardScaler().fit(X_trainw)\n",
        "X_trainw = scalerw.transform(X_trainw)\n",
        "X_testw  = scalerw.transform(X_testw)\n",
        "todayw = copy.copy(scalerw.transform(todayData))\n",
        "#month\n",
        "scalerm = preprocessing.StandardScaler().fit(X_trainm)\n",
        "X_trainm = scalerm.transform(X_trainm)\n",
        "X_testm  = scalerm.transform(X_testm)\n",
        "todaym = copy.copy(scalerm.transform(todayData))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYj3Sofv-HYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#early stopping\n",
        "patience=20\n",
        "early_stoppingd = EarlyStopping(patience=patience, verbose=1)\n",
        "checkpointerd = ModelCheckpoint(filepath='weightsd.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "early_stoppingw = EarlyStopping(patience=patience, verbose=1)\n",
        "checkpointerw = ModelCheckpoint(filepath='weightsw.hdf5', save_best_only=True, verbose=1)\n",
        "\n",
        "early_stoppingm = EarlyStopping(patience=patience, verbose=1)\n",
        "checkpointerm = ModelCheckpoint(filepath='weightsm.hdf5', save_best_only=True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TvO2dB415j9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d436bf0d-a5d2-48dd-cc2c-0be7ad2d75b7"
      },
      "source": [
        "#defining the model and training for NEXT DAY\n",
        "modeld = Sequential()\n",
        "modeld.add(Dense(128, activation='relu', input_dim=(len(X_traind[0]))))\n",
        "modeld.add(Dense(64, activation='relu'))\n",
        "modeld.add(Dense(64, activation='relu'))\n",
        "modeld.add(Dropout(0.1))\n",
        "modeld.add(Dense(64, activation='relu'))\n",
        "modeld.add(Dense(1))\n",
        "\n",
        "modeld.compile(loss='MSE',\n",
        "              optimizer='adam')\n",
        "\n",
        "historyd = modeld.fit(X_traind,Y_traind,\n",
        "          epochs=1000,batch_size=128,validation_split=0.2,\n",
        "          callbacks=[checkpointerd, early_stoppingd])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 3164 samples, validate on 791 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "3164/3164 [==============================] - 1s 387us/step - loss: 180.2410 - val_loss: 101.9454\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 101.94540, saving model to weightsd.hdf5\n",
            "Epoch 2/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 44.2853 - val_loss: 18.5345\n",
            "\n",
            "Epoch 00002: val_loss improved from 101.94540 to 18.53445, saving model to weightsd.hdf5\n",
            "Epoch 3/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 15.5459 - val_loss: 11.4019\n",
            "\n",
            "Epoch 00003: val_loss improved from 18.53445 to 11.40194, saving model to weightsd.hdf5\n",
            "Epoch 4/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 10.3610 - val_loss: 9.2848\n",
            "\n",
            "Epoch 00004: val_loss improved from 11.40194 to 9.28477, saving model to weightsd.hdf5\n",
            "Epoch 5/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 9.0194 - val_loss: 8.1913\n",
            "\n",
            "Epoch 00005: val_loss improved from 9.28477 to 8.19127, saving model to weightsd.hdf5\n",
            "Epoch 6/100\n",
            "3164/3164 [==============================] - 0s 52us/step - loss: 7.8749 - val_loss: 7.1811\n",
            "\n",
            "Epoch 00006: val_loss improved from 8.19127 to 7.18108, saving model to weightsd.hdf5\n",
            "Epoch 7/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 7.1173 - val_loss: 6.5268\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.18108 to 6.52680, saving model to weightsd.hdf5\n",
            "Epoch 8/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 6.4286 - val_loss: 6.0799\n",
            "\n",
            "Epoch 00008: val_loss improved from 6.52680 to 6.07989, saving model to weightsd.hdf5\n",
            "Epoch 9/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 6.1982 - val_loss: 5.7386\n",
            "\n",
            "Epoch 00009: val_loss improved from 6.07989 to 5.73856, saving model to weightsd.hdf5\n",
            "Epoch 10/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 5.7915 - val_loss: 5.6110\n",
            "\n",
            "Epoch 00010: val_loss improved from 5.73856 to 5.61100, saving model to weightsd.hdf5\n",
            "Epoch 11/100\n",
            "3164/3164 [==============================] - 0s 42us/step - loss: 5.7454 - val_loss: 5.9265\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 5.61100\n",
            "Epoch 12/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 5.4245 - val_loss: 5.3937\n",
            "\n",
            "Epoch 00012: val_loss improved from 5.61100 to 5.39373, saving model to weightsd.hdf5\n",
            "Epoch 13/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 5.3658 - val_loss: 5.3091\n",
            "\n",
            "Epoch 00013: val_loss improved from 5.39373 to 5.30906, saving model to weightsd.hdf5\n",
            "Epoch 14/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 5.2546 - val_loss: 5.1219\n",
            "\n",
            "Epoch 00014: val_loss improved from 5.30906 to 5.12191, saving model to weightsd.hdf5\n",
            "Epoch 15/100\n",
            "3164/3164 [==============================] - 0s 44us/step - loss: 5.1881 - val_loss: 5.0578\n",
            "\n",
            "Epoch 00015: val_loss improved from 5.12191 to 5.05782, saving model to weightsd.hdf5\n",
            "Epoch 16/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 5.2134 - val_loss: 5.1088\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 5.05782\n",
            "Epoch 17/100\n",
            "3164/3164 [==============================] - 0s 47us/step - loss: 5.1884 - val_loss: 5.2311\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 5.05782\n",
            "Epoch 18/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 4.9605 - val_loss: 5.1538\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 5.05782\n",
            "Epoch 19/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 4.8331 - val_loss: 5.0692\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 5.05782\n",
            "Epoch 20/100\n",
            "3164/3164 [==============================] - 0s 50us/step - loss: 4.8463 - val_loss: 5.1727\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.05782\n",
            "Epoch 21/100\n",
            "3164/3164 [==============================] - 0s 42us/step - loss: 4.9194 - val_loss: 5.0932\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 5.05782\n",
            "Epoch 22/100\n",
            "3164/3164 [==============================] - 0s 42us/step - loss: 4.9660 - val_loss: 4.9372\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.05782 to 4.93721, saving model to weightsd.hdf5\n",
            "Epoch 23/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 4.7773 - val_loss: 4.7806\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.93721 to 4.78062, saving model to weightsd.hdf5\n",
            "Epoch 24/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 4.6212 - val_loss: 4.8190\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 4.78062\n",
            "Epoch 25/100\n",
            "3164/3164 [==============================] - 0s 47us/step - loss: 4.7795 - val_loss: 4.6817\n",
            "\n",
            "Epoch 00025: val_loss improved from 4.78062 to 4.68173, saving model to weightsd.hdf5\n",
            "Epoch 26/100\n",
            "3164/3164 [==============================] - 0s 62us/step - loss: 4.7591 - val_loss: 4.8962\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 4.68173\n",
            "Epoch 27/100\n",
            "3164/3164 [==============================] - 0s 51us/step - loss: 4.7390 - val_loss: 5.0349\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 4.68173\n",
            "Epoch 28/100\n",
            "3164/3164 [==============================] - 0s 52us/step - loss: 4.7302 - val_loss: 4.8136\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 4.68173\n",
            "Epoch 29/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.6692 - val_loss: 4.7154\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 4.68173\n",
            "Epoch 30/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 4.5391 - val_loss: 4.8321\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 4.68173\n",
            "Epoch 31/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 4.8110 - val_loss: 4.7337\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 4.68173\n",
            "Epoch 32/100\n",
            "3164/3164 [==============================] - 0s 51us/step - loss: 4.6811 - val_loss: 4.8132\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 4.68173\n",
            "Epoch 33/100\n",
            "3164/3164 [==============================] - 0s 47us/step - loss: 4.4954 - val_loss: 5.0277\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 4.68173\n",
            "Epoch 34/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 4.6472 - val_loss: 4.7933\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 4.68173\n",
            "Epoch 35/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 4.6255 - val_loss: 4.6732\n",
            "\n",
            "Epoch 00035: val_loss improved from 4.68173 to 4.67324, saving model to weightsd.hdf5\n",
            "Epoch 36/100\n",
            "3164/3164 [==============================] - 0s 47us/step - loss: 4.6709 - val_loss: 4.6590\n",
            "\n",
            "Epoch 00036: val_loss improved from 4.67324 to 4.65904, saving model to weightsd.hdf5\n",
            "Epoch 37/100\n",
            "3164/3164 [==============================] - 0s 52us/step - loss: 4.5924 - val_loss: 4.7540\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 4.65904\n",
            "Epoch 38/100\n",
            "3164/3164 [==============================] - 0s 60us/step - loss: 4.4961 - val_loss: 4.7524\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 4.65904\n",
            "Epoch 39/100\n",
            "3164/3164 [==============================] - 0s 52us/step - loss: 4.4370 - val_loss: 4.6915\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 4.65904\n",
            "Epoch 40/100\n",
            "3164/3164 [==============================] - 0s 54us/step - loss: 4.4875 - val_loss: 4.6715\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 4.65904\n",
            "Epoch 41/100\n",
            "3164/3164 [==============================] - 0s 49us/step - loss: 4.5734 - val_loss: 4.7849\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 4.65904\n",
            "Epoch 42/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.4804 - val_loss: 4.7004\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 4.65904\n",
            "Epoch 43/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.3511 - val_loss: 4.8701\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 4.65904\n",
            "Epoch 44/100\n",
            "3164/3164 [==============================] - 0s 49us/step - loss: 4.4521 - val_loss: 4.8856\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 4.65904\n",
            "Epoch 45/100\n",
            "3164/3164 [==============================] - 0s 52us/step - loss: 4.3469 - val_loss: 4.6588\n",
            "\n",
            "Epoch 00045: val_loss improved from 4.65904 to 4.65884, saving model to weightsd.hdf5\n",
            "Epoch 46/100\n",
            "3164/3164 [==============================] - 0s 57us/step - loss: 4.5334 - val_loss: 4.6891\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 4.65884\n",
            "Epoch 47/100\n",
            "3164/3164 [==============================] - 0s 49us/step - loss: 4.6180 - val_loss: 4.8286\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 4.65884\n",
            "Epoch 48/100\n",
            "3164/3164 [==============================] - 0s 45us/step - loss: 4.3847 - val_loss: 4.7486\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 4.65884\n",
            "Epoch 49/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 4.3339 - val_loss: 4.7152\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 4.65884\n",
            "Epoch 50/100\n",
            "3164/3164 [==============================] - 0s 44us/step - loss: 4.4235 - val_loss: 5.2020\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 4.65884\n",
            "Epoch 51/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 4.4973 - val_loss: 5.0531\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 4.65884\n",
            "Epoch 52/100\n",
            "3164/3164 [==============================] - 0s 44us/step - loss: 4.4469 - val_loss: 4.7160\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 4.65884\n",
            "Epoch 53/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 4.3380 - val_loss: 4.7854\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 4.65884\n",
            "Epoch 54/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 4.3771 - val_loss: 4.8454\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 4.65884\n",
            "Epoch 55/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.4399 - val_loss: 4.6960\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 4.65884\n",
            "Epoch 56/100\n",
            "3164/3164 [==============================] - 0s 51us/step - loss: 4.2329 - val_loss: 4.6905\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 4.65884\n",
            "Epoch 57/100\n",
            "3164/3164 [==============================] - 0s 50us/step - loss: 4.2662 - val_loss: 4.7274\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 4.65884\n",
            "Epoch 58/100\n",
            "3164/3164 [==============================] - 0s 54us/step - loss: 4.3975 - val_loss: 4.7284\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 4.65884\n",
            "Epoch 59/100\n",
            "3164/3164 [==============================] - 0s 51us/step - loss: 4.3689 - val_loss: 4.6791\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 4.65884\n",
            "Epoch 60/100\n",
            "3164/3164 [==============================] - 0s 55us/step - loss: 4.3216 - val_loss: 4.7169\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 4.65884\n",
            "Epoch 61/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 4.2049 - val_loss: 4.6070\n",
            "\n",
            "Epoch 00061: val_loss improved from 4.65884 to 4.60698, saving model to weightsd.hdf5\n",
            "Epoch 62/100\n",
            "3164/3164 [==============================] - 0s 51us/step - loss: 4.2146 - val_loss: 4.9084\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 4.60698\n",
            "Epoch 63/100\n",
            "3164/3164 [==============================] - 0s 54us/step - loss: 4.2164 - val_loss: 4.8150\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 4.60698\n",
            "Epoch 64/100\n",
            "3164/3164 [==============================] - 0s 60us/step - loss: 4.2266 - val_loss: 4.7448\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 4.60698\n",
            "Epoch 65/100\n",
            "3164/3164 [==============================] - 0s 66us/step - loss: 4.2850 - val_loss: 4.7184\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 4.60698\n",
            "Epoch 66/100\n",
            "3164/3164 [==============================] - 0s 47us/step - loss: 4.3610 - val_loss: 5.5898\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 4.60698\n",
            "Epoch 67/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 4.2756 - val_loss: 4.7710\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 4.60698\n",
            "Epoch 68/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 4.2830 - val_loss: 4.8621\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 4.60698\n",
            "Epoch 69/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 4.2316 - val_loss: 4.9020\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 4.60698\n",
            "Epoch 70/100\n",
            "3164/3164 [==============================] - 0s 49us/step - loss: 4.1752 - val_loss: 4.6219\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 4.60698\n",
            "Epoch 71/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.1217 - val_loss: 4.6668\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 4.60698\n",
            "Epoch 72/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 4.2973 - val_loss: 5.0803\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 4.60698\n",
            "Epoch 73/100\n",
            "3164/3164 [==============================] - 0s 48us/step - loss: 4.3732 - val_loss: 4.7252\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 4.60698\n",
            "Epoch 74/100\n",
            "3164/3164 [==============================] - 0s 44us/step - loss: 4.1691 - val_loss: 4.9681\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 4.60698\n",
            "Epoch 75/100\n",
            "3164/3164 [==============================] - 0s 43us/step - loss: 4.2560 - val_loss: 4.6377\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 4.60698\n",
            "Epoch 76/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.2545 - val_loss: 5.2334\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 4.60698\n",
            "Epoch 77/100\n",
            "3164/3164 [==============================] - 0s 49us/step - loss: 4.1615 - val_loss: 4.8089\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 4.60698\n",
            "Epoch 78/100\n",
            "3164/3164 [==============================] - 0s 47us/step - loss: 4.2303 - val_loss: 4.6792\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 4.60698\n",
            "Epoch 79/100\n",
            "3164/3164 [==============================] - 0s 50us/step - loss: 4.0963 - val_loss: 4.7941\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 4.60698\n",
            "Epoch 80/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.2197 - val_loss: 4.9936\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 4.60698\n",
            "Epoch 81/100\n",
            "3164/3164 [==============================] - 0s 46us/step - loss: 4.1749 - val_loss: 4.9806\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 4.60698\n",
            "Epoch 00081: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o82aJElq2E1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b684ecc-6cd6-4ff3-a8a5-3645387faa5d"
      },
      "source": [
        "#Defining model and training for NEXT WEEK\n",
        "\n",
        "modelw = Sequential()\n",
        "modelw.add(Dense(128, activation='sigmoid', input_dim=(len(X_trainw[0]))))\n",
        "modelw.add(Dense(64, activation='relu'))\n",
        "modelw.add(Dense(64, activation='relu'))\n",
        "modelw.add(Dense(64, activation='relu'))\n",
        "modelw.add(Dense(1))\n",
        "\n",
        "modelw.compile(loss='MSE',\n",
        "              optimizer='adam')\n",
        "\n",
        "historyw = modelw.fit(X_trainw,Y_trainw,\n",
        "          epochs=1000,batch_size=128,validation_split=0.2,\n",
        "          callbacks=[checkpointerw, early_stoppingw])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3159 samples, validate on 790 samples\n",
            "Epoch 1/1000\n",
            "3159/3159 [==============================] - 1s 168us/step - loss: 154.2978 - val_loss: 71.6487\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 71.64870, saving model to weightsw.hdf5\n",
            "Epoch 2/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 72.3721 - val_loss: 60.5605\n",
            "\n",
            "Epoch 00002: val_loss improved from 71.64870 to 60.56052, saving model to weightsw.hdf5\n",
            "Epoch 3/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 60.9247 - val_loss: 50.1562\n",
            "\n",
            "Epoch 00003: val_loss improved from 60.56052 to 50.15623, saving model to weightsw.hdf5\n",
            "Epoch 4/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 49.1484 - val_loss: 37.2367\n",
            "\n",
            "Epoch 00004: val_loss improved from 50.15623 to 37.23670, saving model to weightsw.hdf5\n",
            "Epoch 5/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 32.2748 - val_loss: 20.9729\n",
            "\n",
            "Epoch 00005: val_loss improved from 37.23670 to 20.97292, saving model to weightsw.hdf5\n",
            "Epoch 6/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 21.1260 - val_loss: 19.1475\n",
            "\n",
            "Epoch 00006: val_loss improved from 20.97292 to 19.14752, saving model to weightsw.hdf5\n",
            "Epoch 7/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 20.0374 - val_loss: 18.9048\n",
            "\n",
            "Epoch 00007: val_loss improved from 19.14752 to 18.90475, saving model to weightsw.hdf5\n",
            "Epoch 8/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 20.0336 - val_loss: 18.6687\n",
            "\n",
            "Epoch 00008: val_loss improved from 18.90475 to 18.66867, saving model to weightsw.hdf5\n",
            "Epoch 9/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 19.6213 - val_loss: 18.5531\n",
            "\n",
            "Epoch 00009: val_loss improved from 18.66867 to 18.55311, saving model to weightsw.hdf5\n",
            "Epoch 10/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 19.5354 - val_loss: 18.6782\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 18.55311\n",
            "Epoch 11/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 19.4363 - val_loss: 18.5071\n",
            "\n",
            "Epoch 00011: val_loss improved from 18.55311 to 18.50706, saving model to weightsw.hdf5\n",
            "Epoch 12/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 19.3825 - val_loss: 18.4005\n",
            "\n",
            "Epoch 00012: val_loss improved from 18.50706 to 18.40051, saving model to weightsw.hdf5\n",
            "Epoch 13/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 19.6641 - val_loss: 18.4675\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 18.40051\n",
            "Epoch 14/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 19.5102 - val_loss: 18.3970\n",
            "\n",
            "Epoch 00014: val_loss improved from 18.40051 to 18.39705, saving model to weightsw.hdf5\n",
            "Epoch 15/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 19.4498 - val_loss: 18.8334\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 18.39705\n",
            "Epoch 16/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 19.3370 - val_loss: 18.5831\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 18.39705\n",
            "Epoch 17/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 19.2387 - val_loss: 19.4555\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 18.39705\n",
            "Epoch 18/1000\n",
            "3159/3159 [==============================] - 0s 49us/step - loss: 19.3371 - val_loss: 18.5029\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 18.39705\n",
            "Epoch 19/1000\n",
            "3159/3159 [==============================] - 0s 54us/step - loss: 19.2799 - val_loss: 18.3534\n",
            "\n",
            "Epoch 00019: val_loss improved from 18.39705 to 18.35338, saving model to weightsw.hdf5\n",
            "Epoch 20/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 19.5634 - val_loss: 18.4509\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 18.35338\n",
            "Epoch 21/1000\n",
            "3159/3159 [==============================] - 0s 49us/step - loss: 19.2535 - val_loss: 19.4392\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 18.35338\n",
            "Epoch 22/1000\n",
            "3159/3159 [==============================] - 0s 55us/step - loss: 19.5929 - val_loss: 18.7725\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 18.35338\n",
            "Epoch 23/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 19.2353 - val_loss: 18.2979\n",
            "\n",
            "Epoch 00023: val_loss improved from 18.35338 to 18.29795, saving model to weightsw.hdf5\n",
            "Epoch 24/1000\n",
            "3159/3159 [==============================] - 0s 53us/step - loss: 19.2688 - val_loss: 18.2594\n",
            "\n",
            "Epoch 00024: val_loss improved from 18.29795 to 18.25938, saving model to weightsw.hdf5\n",
            "Epoch 25/1000\n",
            "3159/3159 [==============================] - 0s 55us/step - loss: 19.3560 - val_loss: 18.3130\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 18.25938\n",
            "Epoch 26/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 19.1907 - val_loss: 18.2435\n",
            "\n",
            "Epoch 00026: val_loss improved from 18.25938 to 18.24354, saving model to weightsw.hdf5\n",
            "Epoch 27/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 19.1462 - val_loss: 18.1361\n",
            "\n",
            "Epoch 00027: val_loss improved from 18.24354 to 18.13607, saving model to weightsw.hdf5\n",
            "Epoch 28/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 19.0450 - val_loss: 18.0698\n",
            "\n",
            "Epoch 00028: val_loss improved from 18.13607 to 18.06979, saving model to weightsw.hdf5\n",
            "Epoch 29/1000\n",
            "3159/3159 [==============================] - 0s 52us/step - loss: 19.2702 - val_loss: 18.3193\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 18.06979\n",
            "Epoch 30/1000\n",
            "3159/3159 [==============================] - 0s 51us/step - loss: 19.0452 - val_loss: 18.3425\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 18.06979\n",
            "Epoch 31/1000\n",
            "3159/3159 [==============================] - 0s 54us/step - loss: 19.0482 - val_loss: 18.0506\n",
            "\n",
            "Epoch 00031: val_loss improved from 18.06979 to 18.05063, saving model to weightsw.hdf5\n",
            "Epoch 32/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 19.0714 - val_loss: 18.0599\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 18.05063\n",
            "Epoch 33/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.9449 - val_loss: 18.1572\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 18.05063\n",
            "Epoch 34/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 18.9990 - val_loss: 18.1886\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 18.05063\n",
            "Epoch 35/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.9724 - val_loss: 18.5041\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 18.05063\n",
            "Epoch 36/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 19.0400 - val_loss: 18.2881\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 18.05063\n",
            "Epoch 37/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 18.9838 - val_loss: 18.1694\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 18.05063\n",
            "Epoch 38/1000\n",
            "3159/3159 [==============================] - 0s 49us/step - loss: 19.0558 - val_loss: 18.7141\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 18.05063\n",
            "Epoch 39/1000\n",
            "3159/3159 [==============================] - 0s 58us/step - loss: 19.2422 - val_loss: 18.0736\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 18.05063\n",
            "Epoch 40/1000\n",
            "3159/3159 [==============================] - 0s 51us/step - loss: 19.1133 - val_loss: 18.1417\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 18.05063\n",
            "Epoch 41/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.9260 - val_loss: 18.3888\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 18.05063\n",
            "Epoch 42/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.8362 - val_loss: 17.9669\n",
            "\n",
            "Epoch 00042: val_loss improved from 18.05063 to 17.96692, saving model to weightsw.hdf5\n",
            "Epoch 43/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 18.8623 - val_loss: 17.9497\n",
            "\n",
            "Epoch 00043: val_loss improved from 17.96692 to 17.94972, saving model to weightsw.hdf5\n",
            "Epoch 44/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 18.8976 - val_loss: 18.0767\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 17.94972\n",
            "Epoch 45/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 18.9856 - val_loss: 17.9602\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 17.94972\n",
            "Epoch 46/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 18.9348 - val_loss: 17.9109\n",
            "\n",
            "Epoch 00046: val_loss improved from 17.94972 to 17.91089, saving model to weightsw.hdf5\n",
            "Epoch 47/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.7960 - val_loss: 18.0543\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 17.91089\n",
            "Epoch 48/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 18.8287 - val_loss: 18.0829\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 17.91089\n",
            "Epoch 49/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.8518 - val_loss: 17.9921\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 17.91089\n",
            "Epoch 50/1000\n",
            "3159/3159 [==============================] - 0s 50us/step - loss: 18.7652 - val_loss: 17.9125\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 17.91089\n",
            "Epoch 51/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 18.8001 - val_loss: 18.0536\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 17.91089\n",
            "Epoch 52/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.7192 - val_loss: 19.3453\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 17.91089\n",
            "Epoch 53/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.9400 - val_loss: 18.1231\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 17.91089\n",
            "Epoch 54/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 18.9343 - val_loss: 17.8499\n",
            "\n",
            "Epoch 00054: val_loss improved from 17.91089 to 17.84988, saving model to weightsw.hdf5\n",
            "Epoch 55/1000\n",
            "3159/3159 [==============================] - 0s 51us/step - loss: 18.8891 - val_loss: 17.9173\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 17.84988\n",
            "Epoch 56/1000\n",
            "3159/3159 [==============================] - 0s 49us/step - loss: 18.8650 - val_loss: 18.0745\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 17.84988\n",
            "Epoch 57/1000\n",
            "3159/3159 [==============================] - 0s 51us/step - loss: 18.8459 - val_loss: 18.1015\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 17.84988\n",
            "Epoch 58/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 18.6987 - val_loss: 18.0016\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 17.84988\n",
            "Epoch 59/1000\n",
            "3159/3159 [==============================] - 0s 50us/step - loss: 18.6267 - val_loss: 17.8238\n",
            "\n",
            "Epoch 00059: val_loss improved from 17.84988 to 17.82377, saving model to weightsw.hdf5\n",
            "Epoch 60/1000\n",
            "3159/3159 [==============================] - 0s 52us/step - loss: 18.6897 - val_loss: 17.9245\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 17.82377\n",
            "Epoch 61/1000\n",
            "3159/3159 [==============================] - 0s 52us/step - loss: 18.6194 - val_loss: 17.8303\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 17.82377\n",
            "Epoch 62/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 18.8004 - val_loss: 18.0356\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 17.82377\n",
            "Epoch 63/1000\n",
            "3159/3159 [==============================] - 0s 49us/step - loss: 18.5457 - val_loss: 17.9084\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 17.82377\n",
            "Epoch 64/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.7127 - val_loss: 17.9243\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 17.82377\n",
            "Epoch 65/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 18.5863 - val_loss: 18.0282\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 17.82377\n",
            "Epoch 66/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.5344 - val_loss: 17.8820\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 17.82377\n",
            "Epoch 67/1000\n",
            "3159/3159 [==============================] - 0s 41us/step - loss: 18.5505 - val_loss: 17.9532\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 17.82377\n",
            "Epoch 68/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.4987 - val_loss: 18.1081\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 17.82377\n",
            "Epoch 69/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 18.4418 - val_loss: 17.8370\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 17.82377\n",
            "Epoch 70/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 18.4208 - val_loss: 17.9599\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 17.82377\n",
            "Epoch 71/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 18.4217 - val_loss: 18.0509\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 17.82377\n",
            "Epoch 72/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 18.5074 - val_loss: 17.9792\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 17.82377\n",
            "Epoch 73/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 18.5272 - val_loss: 18.4010\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 17.82377\n",
            "Epoch 74/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 18.5479 - val_loss: 19.0861\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 17.82377\n",
            "Epoch 75/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 18.5358 - val_loss: 17.7174\n",
            "\n",
            "Epoch 00075: val_loss improved from 17.82377 to 17.71740, saving model to weightsw.hdf5\n",
            "Epoch 76/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.3268 - val_loss: 18.1073\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 17.71740\n",
            "Epoch 77/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.5143 - val_loss: 17.9611\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 17.71740\n",
            "Epoch 78/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 18.3198 - val_loss: 17.8698\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 17.71740\n",
            "Epoch 79/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 18.2396 - val_loss: 17.6003\n",
            "\n",
            "Epoch 00079: val_loss improved from 17.71740 to 17.60033, saving model to weightsw.hdf5\n",
            "Epoch 80/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.2757 - val_loss: 17.6456\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 17.60033\n",
            "Epoch 81/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 18.0504 - val_loss: 17.5703\n",
            "\n",
            "Epoch 00081: val_loss improved from 17.60033 to 17.57025, saving model to weightsw.hdf5\n",
            "Epoch 82/1000\n",
            "3159/3159 [==============================] - 0s 41us/step - loss: 17.9820 - val_loss: 17.4716\n",
            "\n",
            "Epoch 00082: val_loss improved from 17.57025 to 17.47163, saving model to weightsw.hdf5\n",
            "Epoch 83/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 17.7139 - val_loss: 17.2750\n",
            "\n",
            "Epoch 00083: val_loss improved from 17.47163 to 17.27505, saving model to weightsw.hdf5\n",
            "Epoch 84/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 17.6286 - val_loss: 17.0277\n",
            "\n",
            "Epoch 00084: val_loss improved from 17.27505 to 17.02768, saving model to weightsw.hdf5\n",
            "Epoch 85/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 17.6073 - val_loss: 16.8217\n",
            "\n",
            "Epoch 00085: val_loss improved from 17.02768 to 16.82172, saving model to weightsw.hdf5\n",
            "Epoch 86/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 17.3309 - val_loss: 18.4476\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 16.82172\n",
            "Epoch 87/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 17.1520 - val_loss: 16.6910\n",
            "\n",
            "Epoch 00087: val_loss improved from 16.82172 to 16.69102, saving model to weightsw.hdf5\n",
            "Epoch 88/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 17.1150 - val_loss: 16.2288\n",
            "\n",
            "Epoch 00088: val_loss improved from 16.69102 to 16.22880, saving model to weightsw.hdf5\n",
            "Epoch 89/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 16.7760 - val_loss: 16.4040\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 16.22880\n",
            "Epoch 90/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 16.6299 - val_loss: 16.0551\n",
            "\n",
            "Epoch 00090: val_loss improved from 16.22880 to 16.05510, saving model to weightsw.hdf5\n",
            "Epoch 91/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 16.5790 - val_loss: 15.8107\n",
            "\n",
            "Epoch 00091: val_loss improved from 16.05510 to 15.81070, saving model to weightsw.hdf5\n",
            "Epoch 92/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 16.1252 - val_loss: 15.1700\n",
            "\n",
            "Epoch 00092: val_loss improved from 15.81070 to 15.17003, saving model to weightsw.hdf5\n",
            "Epoch 93/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 15.9668 - val_loss: 14.9032\n",
            "\n",
            "Epoch 00093: val_loss improved from 15.17003 to 14.90324, saving model to weightsw.hdf5\n",
            "Epoch 94/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 15.5737 - val_loss: 15.0755\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 14.90324\n",
            "Epoch 95/1000\n",
            "3159/3159 [==============================] - 0s 50us/step - loss: 15.5884 - val_loss: 15.4913\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 14.90324\n",
            "Epoch 96/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 15.6199 - val_loss: 14.4082\n",
            "\n",
            "Epoch 00096: val_loss improved from 14.90324 to 14.40821, saving model to weightsw.hdf5\n",
            "Epoch 97/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 15.2540 - val_loss: 14.1725\n",
            "\n",
            "Epoch 00097: val_loss improved from 14.40821 to 14.17250, saving model to weightsw.hdf5\n",
            "Epoch 98/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 15.3108 - val_loss: 14.7192\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 14.17250\n",
            "Epoch 99/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 15.2939 - val_loss: 14.5191\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 14.17250\n",
            "Epoch 100/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 15.2158 - val_loss: 13.9286\n",
            "\n",
            "Epoch 00100: val_loss improved from 14.17250 to 13.92861, saving model to weightsw.hdf5\n",
            "Epoch 101/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 14.7783 - val_loss: 14.0422\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 13.92861\n",
            "Epoch 102/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 14.6831 - val_loss: 13.8763\n",
            "\n",
            "Epoch 00102: val_loss improved from 13.92861 to 13.87633, saving model to weightsw.hdf5\n",
            "Epoch 103/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 14.4927 - val_loss: 13.7647\n",
            "\n",
            "Epoch 00103: val_loss improved from 13.87633 to 13.76466, saving model to weightsw.hdf5\n",
            "Epoch 104/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 14.6939 - val_loss: 13.3439\n",
            "\n",
            "Epoch 00104: val_loss improved from 13.76466 to 13.34391, saving model to weightsw.hdf5\n",
            "Epoch 105/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 14.6265 - val_loss: 13.4093\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 13.34391\n",
            "Epoch 106/1000\n",
            "3159/3159 [==============================] - 0s 50us/step - loss: 14.3645 - val_loss: 13.8818\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 13.34391\n",
            "Epoch 107/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 14.4282 - val_loss: 13.2770\n",
            "\n",
            "Epoch 00107: val_loss improved from 13.34391 to 13.27702, saving model to weightsw.hdf5\n",
            "Epoch 108/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 14.2014 - val_loss: 13.0978\n",
            "\n",
            "Epoch 00108: val_loss improved from 13.27702 to 13.09778, saving model to weightsw.hdf5\n",
            "Epoch 109/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 14.2977 - val_loss: 13.1619\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 13.09778\n",
            "Epoch 110/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 14.0299 - val_loss: 13.3482\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 13.09778\n",
            "Epoch 111/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 14.1690 - val_loss: 12.9809\n",
            "\n",
            "Epoch 00111: val_loss improved from 13.09778 to 12.98086, saving model to weightsw.hdf5\n",
            "Epoch 112/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.9886 - val_loss: 14.2540\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 12.98086\n",
            "Epoch 113/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 14.0647 - val_loss: 13.0888\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 12.98086\n",
            "Epoch 114/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 14.0444 - val_loss: 12.9658\n",
            "\n",
            "Epoch 00114: val_loss improved from 12.98086 to 12.96577, saving model to weightsw.hdf5\n",
            "Epoch 115/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 14.0746 - val_loss: 12.6584\n",
            "\n",
            "Epoch 00115: val_loss improved from 12.96577 to 12.65835, saving model to weightsw.hdf5\n",
            "Epoch 116/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.6571 - val_loss: 12.8149\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 12.65835\n",
            "Epoch 117/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.7930 - val_loss: 12.5516\n",
            "\n",
            "Epoch 00117: val_loss improved from 12.65835 to 12.55157, saving model to weightsw.hdf5\n",
            "Epoch 118/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.8321 - val_loss: 12.8413\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 12.55157\n",
            "Epoch 119/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.7632 - val_loss: 13.2606\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 12.55157\n",
            "Epoch 120/1000\n",
            "3159/3159 [==============================] - 0s 52us/step - loss: 13.7051 - val_loss: 12.9695\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 12.55157\n",
            "Epoch 121/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.6639 - val_loss: 12.4310\n",
            "\n",
            "Epoch 00121: val_loss improved from 12.55157 to 12.43096, saving model to weightsw.hdf5\n",
            "Epoch 122/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.6318 - val_loss: 12.7814\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 12.43096\n",
            "Epoch 123/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.7042 - val_loss: 12.4110\n",
            "\n",
            "Epoch 00123: val_loss improved from 12.43096 to 12.41096, saving model to weightsw.hdf5\n",
            "Epoch 124/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.7381 - val_loss: 12.9321\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 12.41096\n",
            "Epoch 125/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 13.6378 - val_loss: 12.3425\n",
            "\n",
            "Epoch 00125: val_loss improved from 12.41096 to 12.34252, saving model to weightsw.hdf5\n",
            "Epoch 126/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 13.5486 - val_loss: 12.6843\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 12.34252\n",
            "Epoch 127/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.4515 - val_loss: 12.7212\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 12.34252\n",
            "Epoch 128/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.5597 - val_loss: 12.3468\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 12.34252\n",
            "Epoch 129/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.4713 - val_loss: 12.2891\n",
            "\n",
            "Epoch 00129: val_loss improved from 12.34252 to 12.28907, saving model to weightsw.hdf5\n",
            "Epoch 130/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.7782 - val_loss: 13.4121\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 12.28907\n",
            "Epoch 131/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.6644 - val_loss: 12.5653\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 12.28907\n",
            "Epoch 132/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.2871 - val_loss: 12.3971\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 12.28907\n",
            "Epoch 133/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.2968 - val_loss: 12.4089\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 12.28907\n",
            "Epoch 134/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 13.3835 - val_loss: 12.9138\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 12.28907\n",
            "Epoch 135/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.4030 - val_loss: 12.6197\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 12.28907\n",
            "Epoch 136/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.3723 - val_loss: 13.0213\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 12.28907\n",
            "Epoch 137/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.6211 - val_loss: 12.6749\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 12.28907\n",
            "Epoch 138/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.6614 - val_loss: 12.7053\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 12.28907\n",
            "Epoch 139/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.6716 - val_loss: 12.4482\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 12.28907\n",
            "Epoch 140/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.3814 - val_loss: 12.2759\n",
            "\n",
            "Epoch 00140: val_loss improved from 12.28907 to 12.27593, saving model to weightsw.hdf5\n",
            "Epoch 141/1000\n",
            "3159/3159 [==============================] - 0s 50us/step - loss: 13.2154 - val_loss: 12.6360\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 12.27593\n",
            "Epoch 142/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.4425 - val_loss: 13.5611\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 12.27593\n",
            "Epoch 143/1000\n",
            "3159/3159 [==============================] - 0s 41us/step - loss: 13.5495 - val_loss: 12.6491\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 12.27593\n",
            "Epoch 144/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.4601 - val_loss: 12.1701\n",
            "\n",
            "Epoch 00144: val_loss improved from 12.27593 to 12.17015, saving model to weightsw.hdf5\n",
            "Epoch 145/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 13.4420 - val_loss: 12.9814\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 12.17015\n",
            "Epoch 146/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 13.5876 - val_loss: 14.2280\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 12.17015\n",
            "Epoch 147/1000\n",
            "3159/3159 [==============================] - 0s 50us/step - loss: 13.3745 - val_loss: 12.2338\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 12.17015\n",
            "Epoch 148/1000\n",
            "3159/3159 [==============================] - 0s 40us/step - loss: 13.1181 - val_loss: 12.5074\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 12.17015\n",
            "Epoch 149/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.2323 - val_loss: 12.2244\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 12.17015\n",
            "Epoch 150/1000\n",
            "3159/3159 [==============================] - 0s 51us/step - loss: 13.2078 - val_loss: 12.9507\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 12.17015\n",
            "Epoch 151/1000\n",
            "3159/3159 [==============================] - 0s 41us/step - loss: 13.2060 - val_loss: 12.1700\n",
            "\n",
            "Epoch 00151: val_loss improved from 12.17015 to 12.17001, saving model to weightsw.hdf5\n",
            "Epoch 152/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.1710 - val_loss: 12.5991\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 12.17001\n",
            "Epoch 153/1000\n",
            "3159/3159 [==============================] - 0s 52us/step - loss: 13.1966 - val_loss: 12.6394\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 12.17001\n",
            "Epoch 154/1000\n",
            "3159/3159 [==============================] - 0s 51us/step - loss: 13.2322 - val_loss: 12.1282\n",
            "\n",
            "Epoch 00154: val_loss improved from 12.17001 to 12.12821, saving model to weightsw.hdf5\n",
            "Epoch 155/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.0435 - val_loss: 12.3595\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 12.12821\n",
            "Epoch 156/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.0345 - val_loss: 12.2339\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 12.12821\n",
            "Epoch 157/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.1048 - val_loss: 12.8200\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 12.12821\n",
            "Epoch 158/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.1006 - val_loss: 12.1895\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 12.12821\n",
            "Epoch 159/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.0810 - val_loss: 12.2588\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 12.12821\n",
            "Epoch 160/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 12.9285 - val_loss: 12.3707\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 12.12821\n",
            "Epoch 161/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 13.2611 - val_loss: 12.2563\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 12.12821\n",
            "Epoch 162/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 13.0935 - val_loss: 12.2256\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 12.12821\n",
            "Epoch 163/1000\n",
            "3159/3159 [==============================] - 0s 48us/step - loss: 13.0983 - val_loss: 12.0857\n",
            "\n",
            "Epoch 00163: val_loss improved from 12.12821 to 12.08572, saving model to weightsw.hdf5\n",
            "Epoch 164/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.1811 - val_loss: 13.1479\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 12.08572\n",
            "Epoch 165/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.0951 - val_loss: 12.0785\n",
            "\n",
            "Epoch 00165: val_loss improved from 12.08572 to 12.07853, saving model to weightsw.hdf5\n",
            "Epoch 166/1000\n",
            "3159/3159 [==============================] - 0s 47us/step - loss: 12.9591 - val_loss: 12.2104\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 12.07853\n",
            "Epoch 167/1000\n",
            "3159/3159 [==============================] - 0s 50us/step - loss: 13.2388 - val_loss: 12.0799\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 12.07853\n",
            "Epoch 168/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 13.1052 - val_loss: 12.1374\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 12.07853\n",
            "Epoch 169/1000\n",
            "3159/3159 [==============================] - 0s 49us/step - loss: 13.0806 - val_loss: 12.3295\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 12.07853\n",
            "Epoch 170/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.0680 - val_loss: 12.1400\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 12.07853\n",
            "Epoch 171/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.0466 - val_loss: 13.2029\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 12.07853\n",
            "Epoch 172/1000\n",
            "3159/3159 [==============================] - 0s 41us/step - loss: 13.1187 - val_loss: 12.4205\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 12.07853\n",
            "Epoch 173/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.3337 - val_loss: 12.3813\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 12.07853\n",
            "Epoch 174/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 12.8812 - val_loss: 12.1606\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 12.07853\n",
            "Epoch 175/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 13.2271 - val_loss: 13.7069\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 12.07853\n",
            "Epoch 176/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.4677 - val_loss: 12.1153\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 12.07853\n",
            "Epoch 177/1000\n",
            "3159/3159 [==============================] - 0s 42us/step - loss: 13.2418 - val_loss: 12.3011\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 12.07853\n",
            "Epoch 178/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.0509 - val_loss: 12.1571\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 12.07853\n",
            "Epoch 179/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 12.8940 - val_loss: 12.1359\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 12.07853\n",
            "Epoch 180/1000\n",
            "3159/3159 [==============================] - 0s 49us/step - loss: 13.0141 - val_loss: 12.1258\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 12.07853\n",
            "Epoch 181/1000\n",
            "3159/3159 [==============================] - 0s 43us/step - loss: 12.9657 - val_loss: 12.2373\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 12.07853\n",
            "Epoch 182/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.1392 - val_loss: 12.1871\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 12.07853\n",
            "Epoch 183/1000\n",
            "3159/3159 [==============================] - 0s 46us/step - loss: 12.9868 - val_loss: 12.6199\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 12.07853\n",
            "Epoch 184/1000\n",
            "3159/3159 [==============================] - 0s 45us/step - loss: 13.0643 - val_loss: 12.0810\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 12.07853\n",
            "Epoch 185/1000\n",
            "3159/3159 [==============================] - 0s 44us/step - loss: 13.1510 - val_loss: 12.1267\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 12.07853\n",
            "Epoch 00185: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WcEgfizXw9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5638fea-be7d-4bba-9933-45b4f33bbeec"
      },
      "source": [
        "#defining and training model for next month\n",
        "modelm = Sequential()\n",
        "modelm.add(Dense(64, activation='sigmoid', input_dim=(len(X_trainm[0]))))\n",
        "modelm.add(Dense(64, activation='relu'))\n",
        "modelm.add(Dense(64, activation='relu'))\n",
        "modelm.add(Dense(64, activation='relu'))\n",
        "modelm.add(Dense(1))\n",
        "\n",
        "modelm.compile(loss='MSE',\n",
        "              optimizer='adam')\n",
        "\n",
        "historym = modelm.fit(X_trainm,Y_trainm,\n",
        "          epochs=1000,batch_size=128,validation_split=0.2,\n",
        "          callbacks=[checkpointerm, early_stoppingm])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3142 samples, validate on 786 samples\n",
            "Epoch 1/1000\n",
            "3142/3142 [==============================] - 1s 209us/step - loss: 174.2297 - val_loss: 96.6575\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 96.65750, saving model to weightsm.hdf5\n",
            "Epoch 2/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 79.2868 - val_loss: 70.4371\n",
            "\n",
            "Epoch 00002: val_loss improved from 96.65750 to 70.43705, saving model to weightsm.hdf5\n",
            "Epoch 3/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 65.0330 - val_loss: 58.9687\n",
            "\n",
            "Epoch 00003: val_loss improved from 70.43705 to 58.96871, saving model to weightsm.hdf5\n",
            "Epoch 4/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 52.6389 - val_loss: 45.3362\n",
            "\n",
            "Epoch 00004: val_loss improved from 58.96871 to 45.33618, saving model to weightsm.hdf5\n",
            "Epoch 5/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 37.8260 - val_loss: 33.7336\n",
            "\n",
            "Epoch 00005: val_loss improved from 45.33618 to 33.73363, saving model to weightsm.hdf5\n",
            "Epoch 6/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 30.8292 - val_loss: 31.2773\n",
            "\n",
            "Epoch 00006: val_loss improved from 33.73363 to 31.27727, saving model to weightsm.hdf5\n",
            "Epoch 7/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 28.9326 - val_loss: 30.0167\n",
            "\n",
            "Epoch 00007: val_loss improved from 31.27727 to 30.01671, saving model to weightsm.hdf5\n",
            "Epoch 8/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 28.2911 - val_loss: 30.1387\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 30.01671\n",
            "Epoch 9/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 28.3370 - val_loss: 28.7542\n",
            "\n",
            "Epoch 00009: val_loss improved from 30.01671 to 28.75418, saving model to weightsm.hdf5\n",
            "Epoch 10/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 27.3603 - val_loss: 28.6681\n",
            "\n",
            "Epoch 00010: val_loss improved from 28.75418 to 28.66808, saving model to weightsm.hdf5\n",
            "Epoch 11/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 27.1479 - val_loss: 28.0269\n",
            "\n",
            "Epoch 00011: val_loss improved from 28.66808 to 28.02690, saving model to weightsm.hdf5\n",
            "Epoch 12/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 26.8681 - val_loss: 27.9265\n",
            "\n",
            "Epoch 00012: val_loss improved from 28.02690 to 27.92648, saving model to weightsm.hdf5\n",
            "Epoch 13/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 26.5980 - val_loss: 27.6771\n",
            "\n",
            "Epoch 00013: val_loss improved from 27.92648 to 27.67715, saving model to weightsm.hdf5\n",
            "Epoch 14/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 26.7339 - val_loss: 27.6054\n",
            "\n",
            "Epoch 00014: val_loss improved from 27.67715 to 27.60537, saving model to weightsm.hdf5\n",
            "Epoch 15/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 26.4120 - val_loss: 27.5582\n",
            "\n",
            "Epoch 00015: val_loss improved from 27.60537 to 27.55820, saving model to weightsm.hdf5\n",
            "Epoch 16/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 26.0427 - val_loss: 27.0743\n",
            "\n",
            "Epoch 00016: val_loss improved from 27.55820 to 27.07433, saving model to weightsm.hdf5\n",
            "Epoch 17/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 25.8664 - val_loss: 27.0753\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 27.07433\n",
            "Epoch 18/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 25.7593 - val_loss: 26.6416\n",
            "\n",
            "Epoch 00018: val_loss improved from 27.07433 to 26.64158, saving model to weightsm.hdf5\n",
            "Epoch 19/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 25.8814 - val_loss: 26.4151\n",
            "\n",
            "Epoch 00019: val_loss improved from 26.64158 to 26.41509, saving model to weightsm.hdf5\n",
            "Epoch 20/1000\n",
            "3142/3142 [==============================] - 0s 42us/step - loss: 25.6555 - val_loss: 26.3221\n",
            "\n",
            "Epoch 00020: val_loss improved from 26.41509 to 26.32205, saving model to weightsm.hdf5\n",
            "Epoch 21/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 25.2503 - val_loss: 26.0571\n",
            "\n",
            "Epoch 00021: val_loss improved from 26.32205 to 26.05711, saving model to weightsm.hdf5\n",
            "Epoch 22/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 24.8952 - val_loss: 26.1359\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 26.05711\n",
            "Epoch 23/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 24.6962 - val_loss: 25.5892\n",
            "\n",
            "Epoch 00023: val_loss improved from 26.05711 to 25.58923, saving model to weightsm.hdf5\n",
            "Epoch 24/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 24.7783 - val_loss: 25.5228\n",
            "\n",
            "Epoch 00024: val_loss improved from 25.58923 to 25.52278, saving model to weightsm.hdf5\n",
            "Epoch 25/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 24.6626 - val_loss: 25.2247\n",
            "\n",
            "Epoch 00025: val_loss improved from 25.52278 to 25.22470, saving model to weightsm.hdf5\n",
            "Epoch 26/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 23.9601 - val_loss: 25.3917\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 25.22470\n",
            "Epoch 27/1000\n",
            "3142/3142 [==============================] - 0s 52us/step - loss: 23.9327 - val_loss: 24.6323\n",
            "\n",
            "Epoch 00027: val_loss improved from 25.22470 to 24.63226, saving model to weightsm.hdf5\n",
            "Epoch 28/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 23.4706 - val_loss: 24.3361\n",
            "\n",
            "Epoch 00028: val_loss improved from 24.63226 to 24.33609, saving model to weightsm.hdf5\n",
            "Epoch 29/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 23.1439 - val_loss: 24.4351\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 24.33609\n",
            "Epoch 30/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 23.1591 - val_loss: 24.2554\n",
            "\n",
            "Epoch 00030: val_loss improved from 24.33609 to 24.25544, saving model to weightsm.hdf5\n",
            "Epoch 31/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 22.9944 - val_loss: 23.6374\n",
            "\n",
            "Epoch 00031: val_loss improved from 24.25544 to 23.63741, saving model to weightsm.hdf5\n",
            "Epoch 32/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 22.0531 - val_loss: 22.8331\n",
            "\n",
            "Epoch 00032: val_loss improved from 23.63741 to 22.83314, saving model to weightsm.hdf5\n",
            "Epoch 33/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 21.7160 - val_loss: 22.4595\n",
            "\n",
            "Epoch 00033: val_loss improved from 22.83314 to 22.45951, saving model to weightsm.hdf5\n",
            "Epoch 34/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 21.0291 - val_loss: 22.0906\n",
            "\n",
            "Epoch 00034: val_loss improved from 22.45951 to 22.09058, saving model to weightsm.hdf5\n",
            "Epoch 35/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 20.4334 - val_loss: 20.8485\n",
            "\n",
            "Epoch 00035: val_loss improved from 22.09058 to 20.84852, saving model to weightsm.hdf5\n",
            "Epoch 36/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 19.7762 - val_loss: 20.3163\n",
            "\n",
            "Epoch 00036: val_loss improved from 20.84852 to 20.31628, saving model to weightsm.hdf5\n",
            "Epoch 37/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 19.1485 - val_loss: 19.2088\n",
            "\n",
            "Epoch 00037: val_loss improved from 20.31628 to 19.20883, saving model to weightsm.hdf5\n",
            "Epoch 38/1000\n",
            "3142/3142 [==============================] - 0s 52us/step - loss: 18.2129 - val_loss: 17.8388\n",
            "\n",
            "Epoch 00038: val_loss improved from 19.20883 to 17.83881, saving model to weightsm.hdf5\n",
            "Epoch 39/1000\n",
            "3142/3142 [==============================] - 0s 51us/step - loss: 17.0035 - val_loss: 17.2132\n",
            "\n",
            "Epoch 00039: val_loss improved from 17.83881 to 17.21324, saving model to weightsm.hdf5\n",
            "Epoch 40/1000\n",
            "3142/3142 [==============================] - 0s 53us/step - loss: 16.3006 - val_loss: 17.1419\n",
            "\n",
            "Epoch 00040: val_loss improved from 17.21324 to 17.14187, saving model to weightsm.hdf5\n",
            "Epoch 41/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 15.9175 - val_loss: 15.8802\n",
            "\n",
            "Epoch 00041: val_loss improved from 17.14187 to 15.88017, saving model to weightsm.hdf5\n",
            "Epoch 42/1000\n",
            "3142/3142 [==============================] - 0s 41us/step - loss: 15.5456 - val_loss: 16.5488\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 15.88017\n",
            "Epoch 43/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 15.8363 - val_loss: 16.0054\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 15.88017\n",
            "Epoch 44/1000\n",
            "3142/3142 [==============================] - 0s 42us/step - loss: 15.6528 - val_loss: 15.5158\n",
            "\n",
            "Epoch 00044: val_loss improved from 15.88017 to 15.51577, saving model to weightsm.hdf5\n",
            "Epoch 45/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 15.2330 - val_loss: 15.1318\n",
            "\n",
            "Epoch 00045: val_loss improved from 15.51577 to 15.13181, saving model to weightsm.hdf5\n",
            "Epoch 46/1000\n",
            "3142/3142 [==============================] - 0s 60us/step - loss: 14.9696 - val_loss: 14.7118\n",
            "\n",
            "Epoch 00046: val_loss improved from 15.13181 to 14.71179, saving model to weightsm.hdf5\n",
            "Epoch 47/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.7602 - val_loss: 14.9056\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 14.71179\n",
            "Epoch 48/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 14.8196 - val_loss: 14.6045\n",
            "\n",
            "Epoch 00048: val_loss improved from 14.71179 to 14.60446, saving model to weightsm.hdf5\n",
            "Epoch 49/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 15.0505 - val_loss: 14.6145\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 14.60446\n",
            "Epoch 50/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 14.6551 - val_loss: 14.4011\n",
            "\n",
            "Epoch 00050: val_loss improved from 14.60446 to 14.40106, saving model to weightsm.hdf5\n",
            "Epoch 51/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.6362 - val_loss: 14.2268\n",
            "\n",
            "Epoch 00051: val_loss improved from 14.40106 to 14.22677, saving model to weightsm.hdf5\n",
            "Epoch 52/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 14.3730 - val_loss: 14.0360\n",
            "\n",
            "Epoch 00052: val_loss improved from 14.22677 to 14.03600, saving model to weightsm.hdf5\n",
            "Epoch 53/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 14.3024 - val_loss: 13.9688\n",
            "\n",
            "Epoch 00053: val_loss improved from 14.03600 to 13.96876, saving model to weightsm.hdf5\n",
            "Epoch 54/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.2731 - val_loss: 14.1408\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 13.96876\n",
            "Epoch 55/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 14.2430 - val_loss: 14.0450\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 13.96876\n",
            "Epoch 56/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 14.4999 - val_loss: 14.7171\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 13.96876\n",
            "Epoch 57/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 14.4232 - val_loss: 14.1707\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 13.96876\n",
            "Epoch 58/1000\n",
            "3142/3142 [==============================] - 0s 53us/step - loss: 14.3798 - val_loss: 14.3031\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 13.96876\n",
            "Epoch 59/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 14.6879 - val_loss: 13.7132\n",
            "\n",
            "Epoch 00059: val_loss improved from 13.96876 to 13.71325, saving model to weightsm.hdf5\n",
            "Epoch 60/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 14.3689 - val_loss: 13.6935\n",
            "\n",
            "Epoch 00060: val_loss improved from 13.71325 to 13.69349, saving model to weightsm.hdf5\n",
            "Epoch 61/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 14.4284 - val_loss: 14.0033\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 13.69349\n",
            "Epoch 62/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.0768 - val_loss: 13.9286\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 13.69349\n",
            "Epoch 63/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 14.1919 - val_loss: 14.2546\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 13.69349\n",
            "Epoch 64/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 14.2100 - val_loss: 13.7078\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 13.69349\n",
            "Epoch 65/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 14.4409 - val_loss: 13.8463\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 13.69349\n",
            "Epoch 66/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 14.6979 - val_loss: 13.8012\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 13.69349\n",
            "Epoch 67/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 14.1738 - val_loss: 13.5215\n",
            "\n",
            "Epoch 00067: val_loss improved from 13.69349 to 13.52150, saving model to weightsm.hdf5\n",
            "Epoch 68/1000\n",
            "3142/3142 [==============================] - 0s 42us/step - loss: 14.5319 - val_loss: 14.4467\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 13.52150\n",
            "Epoch 69/1000\n",
            "3142/3142 [==============================] - 0s 55us/step - loss: 14.3874 - val_loss: 13.6552\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 13.52150\n",
            "Epoch 70/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.0836 - val_loss: 13.6492\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 13.52150\n",
            "Epoch 71/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 14.1480 - val_loss: 14.4943\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 13.52150\n",
            "Epoch 72/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 14.3980 - val_loss: 13.8726\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 13.52150\n",
            "Epoch 73/1000\n",
            "3142/3142 [==============================] - 0s 42us/step - loss: 14.3786 - val_loss: 13.5230\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 13.52150\n",
            "Epoch 74/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 13.9827 - val_loss: 13.5083\n",
            "\n",
            "Epoch 00074: val_loss improved from 13.52150 to 13.50830, saving model to weightsm.hdf5\n",
            "Epoch 75/1000\n",
            "3142/3142 [==============================] - 0s 58us/step - loss: 13.9711 - val_loss: 13.4530\n",
            "\n",
            "Epoch 00075: val_loss improved from 13.50830 to 13.45298, saving model to weightsm.hdf5\n",
            "Epoch 76/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 14.0615 - val_loss: 13.4519\n",
            "\n",
            "Epoch 00076: val_loss improved from 13.45298 to 13.45188, saving model to weightsm.hdf5\n",
            "Epoch 77/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 14.0989 - val_loss: 13.4689\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 13.45188\n",
            "Epoch 78/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 14.2578 - val_loss: 13.9417\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 13.45188\n",
            "Epoch 79/1000\n",
            "3142/3142 [==============================] - 0s 41us/step - loss: 14.1378 - val_loss: 14.1038\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 13.45188\n",
            "Epoch 80/1000\n",
            "3142/3142 [==============================] - 0s 42us/step - loss: 14.2606 - val_loss: 15.4437\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 13.45188\n",
            "Epoch 81/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 14.1156 - val_loss: 13.5695\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 13.45188\n",
            "Epoch 82/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 13.8296 - val_loss: 13.4871\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 13.45188\n",
            "Epoch 83/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 13.8493 - val_loss: 13.3632\n",
            "\n",
            "Epoch 00083: val_loss improved from 13.45188 to 13.36324, saving model to weightsm.hdf5\n",
            "Epoch 84/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 13.9634 - val_loss: 13.8595\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 13.36324\n",
            "Epoch 85/1000\n",
            "3142/3142 [==============================] - 0s 51us/step - loss: 14.0539 - val_loss: 13.7508\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 13.36324\n",
            "Epoch 86/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 14.1508 - val_loss: 13.8188\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 13.36324\n",
            "Epoch 87/1000\n",
            "3142/3142 [==============================] - 0s 50us/step - loss: 13.8584 - val_loss: 13.3548\n",
            "\n",
            "Epoch 00087: val_loss improved from 13.36324 to 13.35481, saving model to weightsm.hdf5\n",
            "Epoch 88/1000\n",
            "3142/3142 [==============================] - 0s 52us/step - loss: 14.0051 - val_loss: 13.4131\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 13.35481\n",
            "Epoch 89/1000\n",
            "3142/3142 [==============================] - 0s 53us/step - loss: 13.8950 - val_loss: 13.3548\n",
            "\n",
            "Epoch 00089: val_loss improved from 13.35481 to 13.35478, saving model to weightsm.hdf5\n",
            "Epoch 90/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 13.9675 - val_loss: 14.1921\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 13.35478\n",
            "Epoch 91/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 14.2872 - val_loss: 13.6342\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 13.35478\n",
            "Epoch 92/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 14.3840 - val_loss: 13.3853\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 13.35478\n",
            "Epoch 93/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 14.0904 - val_loss: 13.4132\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 13.35478\n",
            "Epoch 94/1000\n",
            "3142/3142 [==============================] - 0s 50us/step - loss: 13.9220 - val_loss: 13.5524\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 13.35478\n",
            "Epoch 95/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 13.9746 - val_loss: 13.5956\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 13.35478\n",
            "Epoch 96/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 14.0529 - val_loss: 14.2833\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 13.35478\n",
            "Epoch 97/1000\n",
            "3142/3142 [==============================] - 0s 41us/step - loss: 14.1638 - val_loss: 13.5386\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 13.35478\n",
            "Epoch 98/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 14.2188 - val_loss: 13.5103\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 13.35478\n",
            "Epoch 99/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 13.9050 - val_loss: 13.3964\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 13.35478\n",
            "Epoch 100/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 13.7525 - val_loss: 13.6366\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 13.35478\n",
            "Epoch 101/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.2490 - val_loss: 13.6560\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 13.35478\n",
            "Epoch 102/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 13.8523 - val_loss: 13.4679\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 13.35478\n",
            "Epoch 103/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 13.8430 - val_loss: 13.6113\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 13.35478\n",
            "Epoch 104/1000\n",
            "3142/3142 [==============================] - 0s 42us/step - loss: 14.1082 - val_loss: 13.4366\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 13.35478\n",
            "Epoch 105/1000\n",
            "3142/3142 [==============================] - 0s 50us/step - loss: 14.2437 - val_loss: 14.0526\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 13.35478\n",
            "Epoch 106/1000\n",
            "3142/3142 [==============================] - 0s 41us/step - loss: 14.2549 - val_loss: 13.6298\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 13.35478\n",
            "Epoch 107/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 13.9801 - val_loss: 13.3915\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 13.35478\n",
            "Epoch 108/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 13.7284 - val_loss: 13.3207\n",
            "\n",
            "Epoch 00108: val_loss improved from 13.35478 to 13.32069, saving model to weightsm.hdf5\n",
            "Epoch 109/1000\n",
            "3142/3142 [==============================] - 0s 50us/step - loss: 13.9575 - val_loss: 13.3494\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 13.32069\n",
            "Epoch 110/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 13.7047 - val_loss: 13.4816\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 13.32069\n",
            "Epoch 111/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.0964 - val_loss: 13.2580\n",
            "\n",
            "Epoch 00111: val_loss improved from 13.32069 to 13.25798, saving model to weightsm.hdf5\n",
            "Epoch 112/1000\n",
            "3142/3142 [==============================] - 0s 48us/step - loss: 13.7774 - val_loss: 13.7352\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 13.25798\n",
            "Epoch 113/1000\n",
            "3142/3142 [==============================] - 0s 53us/step - loss: 13.7883 - val_loss: 13.4120\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 13.25798\n",
            "Epoch 114/1000\n",
            "3142/3142 [==============================] - 0s 54us/step - loss: 14.3093 - val_loss: 13.2301\n",
            "\n",
            "Epoch 00114: val_loss improved from 13.25798 to 13.23010, saving model to weightsm.hdf5\n",
            "Epoch 115/1000\n",
            "3142/3142 [==============================] - 0s 53us/step - loss: 14.0051 - val_loss: 13.4505\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 13.23010\n",
            "Epoch 116/1000\n",
            "3142/3142 [==============================] - 0s 53us/step - loss: 13.7674 - val_loss: 13.8472\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 13.23010\n",
            "Epoch 117/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 14.3024 - val_loss: 13.2037\n",
            "\n",
            "Epoch 00117: val_loss improved from 13.23010 to 13.20374, saving model to weightsm.hdf5\n",
            "Epoch 118/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 13.6332 - val_loss: 13.9209\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 13.20374\n",
            "Epoch 119/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 14.0412 - val_loss: 13.1602\n",
            "\n",
            "Epoch 00119: val_loss improved from 13.20374 to 13.16019, saving model to weightsm.hdf5\n",
            "Epoch 120/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 13.6004 - val_loss: 13.4960\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 13.16019\n",
            "Epoch 121/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 13.8613 - val_loss: 13.5992\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 13.16019\n",
            "Epoch 122/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 13.8724 - val_loss: 13.4480\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 13.16019\n",
            "Epoch 123/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 13.6037 - val_loss: 13.2244\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 13.16019\n",
            "Epoch 124/1000\n",
            "3142/3142 [==============================] - 0s 51us/step - loss: 13.6787 - val_loss: 13.6807\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 13.16019\n",
            "Epoch 125/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 14.0230 - val_loss: 14.3988\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 13.16019\n",
            "Epoch 126/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 13.7688 - val_loss: 13.3224\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 13.16019\n",
            "Epoch 127/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 13.8963 - val_loss: 13.2229\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 13.16019\n",
            "Epoch 128/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 13.8753 - val_loss: 13.2509\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 13.16019\n",
            "Epoch 129/1000\n",
            "3142/3142 [==============================] - 0s 44us/step - loss: 13.6602 - val_loss: 13.4026\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 13.16019\n",
            "Epoch 130/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 13.9808 - val_loss: 14.4928\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 13.16019\n",
            "Epoch 131/1000\n",
            "3142/3142 [==============================] - 0s 50us/step - loss: 13.8704 - val_loss: 13.1843\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 13.16019\n",
            "Epoch 132/1000\n",
            "3142/3142 [==============================] - 0s 47us/step - loss: 13.6443 - val_loss: 13.4972\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 13.16019\n",
            "Epoch 133/1000\n",
            "3142/3142 [==============================] - 0s 52us/step - loss: 13.6457 - val_loss: 13.3418\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 13.16019\n",
            "Epoch 134/1000\n",
            "3142/3142 [==============================] - 0s 51us/step - loss: 13.7261 - val_loss: 13.4487\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 13.16019\n",
            "Epoch 135/1000\n",
            "3142/3142 [==============================] - 0s 45us/step - loss: 13.8007 - val_loss: 13.3577\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 13.16019\n",
            "Epoch 136/1000\n",
            "3142/3142 [==============================] - 0s 46us/step - loss: 13.6169 - val_loss: 13.4888\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 13.16019\n",
            "Epoch 137/1000\n",
            "3142/3142 [==============================] - 0s 43us/step - loss: 13.6380 - val_loss: 14.1597\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 13.16019\n",
            "Epoch 138/1000\n",
            "3142/3142 [==============================] - 0s 49us/step - loss: 14.0761 - val_loss: 13.9232\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 13.16019\n",
            "Epoch 139/1000\n",
            "3142/3142 [==============================] - 0s 42us/step - loss: 13.7936 - val_loss: 13.2444\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 13.16019\n",
            "Epoch 00139: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYCdpap-lYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RUN THIS CELL TO LOAD MODEL\n",
        "modeld = load_model('/content/weightsd.hdf5')\n",
        "\n",
        "modelw = load_model('/content/weightsw.hdf5')\n",
        "\n",
        "modelm = load_model('/content/weightsm.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1nqX6z72uJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "27cec92e-58b0-401e-f8db-97e8033fada5"
      },
      "source": [
        "def plot_history(network_history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(network_history.history['loss'])\n",
        "    plt.plot(network_history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "plot_history(historyd)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwX5ni-ZJnaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "44e0020c-b222-43f4-a265-5bbf78436a36"
      },
      "source": [
        "plot_history(historyw)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkSEKBRERGfL/A1qD3EeLH/N0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28NAxRZxJnHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "5e834697-a8c1-4226-910c-0df5d0323f18"
      },
      "source": [
        "plot_history(historym)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgUREemnUBARkX4KBRER6ff/AYmLtOuIypj2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_rNxVZ4pHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3559fab7-5691-4ca8-e0b4-6f2ef7ccd1e2"
      },
      "source": [
        "predd = modeld.predict(todayd)\n",
        "print(predd)\n",
        "\n",
        "predw = modelw.predict(todayw)\n",
        "print(predw)\n",
        "\n",
        "predm = modelm.predict(todaym)\n",
        "print(predm)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.7926354]]\n",
            "[[7.7215395]]\n",
            "[[5.01442]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJQNoLTGet-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}